{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso, regularization and feature selection\n",
    "\n",
    "As reported, for example, by <a href=\"https://cs.nyu.edu/~roweis/csc2515-2006/readings/lasso.pdf\">R.T.</a> or <a href=\"https://stats.stackexchange.com/q/367176\">EDM</a> or <a href=\"https://towardsdatascience.com/feature-selection-using-regularisation-a3678b71e499\"> here </a>:\n",
    "Regularisation consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and in other words to avoid overfitting. In linear model regularisation, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularisation, Lasso or L1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model.\n",
    "\n",
    "Hereafter an example where N-Features/Sensors are created, but only three are correlated to the output. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed module\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score,max_error\n",
    "#set random seed to replicate the results\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creation of the Dataset, input features as collected by different sensors (unnormalised_X)\n",
    "#Output variable, the variable to predict is unnormalised_Y\n",
    "#to avoid \"biases\" we decided to hide the data generating functions ;-) \n",
    "\n",
    "n_samples = 20\n",
    "n_sensors = 8\n",
    "\n",
    "#Train Dataset\n",
    "unnormalised_X = np.array([[0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ,0.64589411, 0.43758721, 0.891773  ],\n",
    "       [0.96366276, 0.38344152, 0.79172504, 0.52889492, 0.56804456, 0.92559664, 0.07103606, 0.0871293 ],\n",
    "       [0.0202184 , 0.83261985, 0.77815675, 0.87001215, 0.97861834, 0.79915856, 0.46147936, 0.78052918],\n",
    "       [0.11827443, 0.63992102, 0.14335329, 0.94466892, 0.52184832, 0.41466194, 0.26455561, 0.77423369],\n",
    "       [0.45615033, 0.56843395, 0.0187898 , 0.6176355 , 0.61209572, 0.616934  , 0.94374808, 0.6818203 ],\n",
    "       [0.3595079 , 0.43703195, 0.6976312 , 0.06022547, 0.66676672, 0.67063787, 0.21038256, 0.1289263 ],\n",
    "       [0.31542835, 0.36371077, 0.57019677, 0.43860151, 0.98837384, 0.10204481, 0.20887676, 0.16130952],\n",
    "       [0.65310833, 0.2532916 , 0.46631077, 0.24442559, 0.15896958, 0.11037514, 0.65632959, 0.13818295],\n",
    "       [0.19658236, 0.36872517, 0.82099323, 0.09710128, 0.83794491, 0.09609841, 0.97645947, 0.4686512 ],\n",
    "       [0.97676109, 0.60484552, 0.73926358, 0.03918779, 0.28280696, 0.12019656, 0.2961402 , 0.11872772],\n",
    "       [0.31798318, 0.41426299, 0.0641475 , 0.69247212, 0.56660145, 0.26538949, 0.52324805, 0.09394051],\n",
    "       [0.5759465 , 0.9292962 , 0.31856895, 0.66741038, 0.13179786, 0.7163272 , 0.28940609, 0.18319136],\n",
    "       [0.58651293, 0.02010755, 0.82894003, 0.00469548, 0.67781654, 0.27000797, 0.73519402, 0.96218855],\n",
    "       [0.24875314, 0.57615733, 0.59204193, 0.57225191, 0.22308163, 0.95274901, 0.44712538, 0.84640867],\n",
    "       [0.69947928, 0.29743695, 0.81379782, 0.39650574, 0.8811032 , 0.58127287, 0.88173536, 0.69253159],\n",
    "       [0.72525428, 0.50132438, 0.95608363, 0.6439902 , 0.42385505, 0.60639321, 0.0191932 , 0.30157482],\n",
    "       [0.66017354, 0.29007761, 0.61801543, 0.4287687 , 0.13547406, 0.29828233, 0.56996491, 0.59087276],\n",
    "       [0.57432525, 0.65320082, 0.65210327, 0.43141844, 0.8965466 , 0.36756187, 0.43586493, 0.89192336],\n",
    "       [0.80619399, 0.70388858, 0.10022689, 0.91948261, 0.7142413 , 0.99884701, 0.1494483 , 0.86812606],\n",
    "       [0.16249293, 0.61555956, 0.12381998, 0.84800823, 0.80731896, 0.56910074, 0.4071833 , 0.069167  ]])\n",
    "\n",
    "unnormalised_y = np.array([[ 0.61073365],[ 0.46434699],[ 1.12708312],[ 1.18590431],[ 0.57131577],[ 0.20110483],[ 0.353339  ],[ 0.12075671],[ 0.0961453 ],[ 0.27434442],[ 0.63432433],\n",
    "                           [ 0.8811441 ],[-0.06344358],[ 0.57083837],[ 0.21776174],[ 0.66346625],[ 0.27188491],[ 0.46913578],[ 1.18244774],[ 0.98617941]])\n",
    "\n",
    "#Test Dataset \n",
    "unnormalised_X_test = np.array([[0.80342804, 0.5275223 , 0.11911147, 0.63968144, 0.09092526,0.33222568, 0.42738095, 0.55438581],\n",
    "       [0.62812652, 0.69739294, 0.78994969, 0.13189035, 0.34277045, 0.20155961, 0.70732423, 0.03339926],\n",
    "       [0.90925004, 0.40516066, 0.76043547, 0.47375838, 0.28671892, 0.75129249, 0.09708994, 0.41235779],\n",
    "       [0.28163896, 0.39027778, 0.87110921, 0.08124512, 0.55793117, 0.54753428, 0.33220307, 0.97326881],\n",
    "       [0.2862761 , 0.5082575 , 0.14795074, 0.19643398, 0.84082001, 0.0037532 , 0.78262101, 0.83347772],\n",
    "       [0.93790734, 0.97260166, 0.83282304, 0.06581761, 0.40379256, 0.37479349, 0.50750135, 0.97787696],\n",
    "       [0.81899021, 0.18754124, 0.69804812, 0.68261077, 0.99909815, 0.48263116, 0.73059268, 0.79518236],\n",
    "       [0.26139168, 0.16107376, 0.69850315, 0.89950917, 0.91515562, 0.31244902, 0.95412616, 0.7242641 ],\n",
    "       [0.02091039, 0.72554552, 0.58165923, 0.9545687 , 0.74233195, 0.19750339, 0.94900651, 0.85836332],\n",
    "       [0.44904621, 0.82365038, 0.99726878, 0.56413064, 0.5890016 , 0.42402702, 0.89548786, 0.44437266],\n",
    "       [0.57723744, 0.66019353, 0.30244304, 0.02295771, 0.83766937, 0.31953292, 0.37552193, 0.18172362],\n",
    "       [0.83135182, 0.18487429, 0.96968683, 0.69644561, 0.60566253, 0.49600661, 0.70888438, 0.26044186],\n",
    "       [0.65267488, 0.62297362, 0.83609334, 0.3572364 , 0.95455374, 0.06336003, 0.86713576, 0.55147501],\n",
    "       [0.8895871 , 0.74683759, 0.20783523, 0.48823397, 0.59087233, 0.87087296, 0.7992923 , 0.95453522],\n",
    "       [0.17335265, 0.106837  , 0.6271594 , 0.37434512, 0.65200627, 0.68657226, 0.47579499, 0.66581477],\n",
    "       [0.95350083, 0.18154475, 0.70021069, 0.44733626, 0.52452094, 0.09235484, 0.67210817, 0.91346847],\n",
    "       [0.33153971, 0.96899058, 0.17409946, 0.05593003, 0.06077135, 0.77983371, 0.56480162, 0.13544652],\n",
    "       [0.48310528, 0.65688242, 0.9341264 , 0.54312395, 0.28354295, 0.21861042, 0.74235601, 0.49747791],\n",
    "       [0.16639362, 0.95924183, 0.35876406, 0.40036286, 0.68637717, 0.9340462 , 0.38488961, 0.35147225],\n",
    "       [0.37233552, 0.17123182, 0.17847354, 0.64228636, 0.97522018, 0.07343925, 0.13086738, 0.81751215]])\n",
    "\n",
    "unnormalised_y_test = np.array([[0.6302154 ],[0.29535911],[0.41731834],[0.16851935],[0.21445296],[0.43988265],[0.48666881],[0.79424102],[1.17907351],[0.64051978],[0.29307163],\n",
    "       [0.50658519],[0.35239108],[0.53186198],[0.14597327],[0.22367129],[0.43114329],[0.54918924],[0.60142237],[0.48506094]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's plot the reading of the different sensors, as function of the output variable \n",
    "\n",
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "for i in range(n_sensors):\n",
    "    ax = plt.subplot(2, int(n_sensors/2), i + 1)\n",
    "    ax.set_title(\"$%i^{th}$ sensor\"%(i+1),fontweight=\"bold\")\n",
    "    ax.scatter(unnormalised_X[:,i],unnormalised_y)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardisation of the inputs and output, i.e. subtracting the mean and dividing by the std-deviation. \n",
    "#it is usually a good pratice to perform standardisation, it makes the ML algorithms work better. \n",
    "scaler_X = StandardScaler().fit(unnormalised_X)\n",
    "scaler_y = StandardScaler().fit(unnormalised_y)\n",
    "X = scaler_X.transform(unnormalised_X)\n",
    "y = scaler_y.transform(unnormalised_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a first Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter default value\n",
    "first_try_alpha = 0.3\n",
    "lasso_model = linear_model.Lasso(alpha=first_try_alpha)\n",
    "lasso_model.fit(X,y)\n",
    "\n",
    "#highlight in red and bold the coeff different from zero\n",
    "for i,c in enumerate(lasso_model.coef_):\n",
    "    text =\"{}th Sensor coeff : {:.2f}\".format(i+1,c)\n",
    "    if(c!=0):\n",
    "        text = \"\\033[1m\"+\"\\033[91m\"+text+\"\\033[0m\"\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The algorithm penalizes tha majority of the features, except for feature 2 and 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For different alphas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to verify that the previous results is not a special outcome given the selected value for alpha,\n",
    "#the hyper-parameter alpha is varied linearly and the model retrained\n",
    "alphas = np.linspace(0.001,1,100)\n",
    "coeffs = np.zeros([100,n_sensors])\n",
    "for i,alpha in enumerate(alphas):\n",
    "    lasso_model = linear_model.Lasso(alpha=alpha)\n",
    "    lasso_model.fit(X,y)\n",
    "    coeffs[i,:] = lasso_model.coef_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also in this case it is clearer that the most important features/sensors are for sure 2,4 and, maybe, 7. \n",
    "#the dashed line shows the value of the alpha for the \"first_try\" model\n",
    "plt.figure(figsize=(17,10))\n",
    "for s in range(n_sensors):\n",
    "    plt.plot(alphas,coeffs[:,s],label=\"$%i^{th}$ sensor\"%(s+1))\n",
    "plt.legend()\n",
    "plt.xlabel('alphas');plt.ylabel('coeff');plt.grid(True)\n",
    "plt.vlines(first_try_alpha,np.min(coeffs),np.max(coeffs),'red',lw=1.5,linestyle=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As example it is shown that a Linear Regression with L2 regularization (Ridge) \n",
    "#will not help in case of feature extraction, \n",
    "#since it does not shrink the coefficient to zero for unuseful features\n",
    "\n",
    "Ralphas = np.linspace(0.001,1000,100)\n",
    "Rcoeffs = np.zeros([100,n_sensors])\n",
    "for i,alpha in enumerate(Ralphas):\n",
    "    ridge_model = linear_model.Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X,y)\n",
    "    Rcoeffs[i,:] = ridge_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(17,10))\n",
    "for s in range(n_sensors):\n",
    "    plt.plot(Ralphas,Rcoeffs[:,s],label=\"$%i^{th}$ sensor\"%(s+1))\n",
    "plt.title(\"Ridge Regression\")\n",
    "plt.legend()\n",
    "plt.xlabel('alphas');plt.ylabel('coeff');plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercize\n",
    "Based on what we have learnt, it can be seen in practice the effect of the regularization (i.e. feature selection), and how to reduce the possible overfitting issue.\n",
    "A new linear model will be trained and evaluated, after the data selection, pre-processing.\n",
    "\n",
    "## Taks\n",
    "* Run the following code and compare the results using:\n",
    "    * all feature (selected sensors = np.arange(8) or [0,1,2,3,4,5,6,7])\n",
    "    * the feature suggested by the Lasso algorithm(the best three): the one that are not immediately penalized by the regularisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the following code twice changing the following line\n",
    "selected_sensors = [...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first see the results on the Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the train sample we want only the selected features/sensors\n",
    "new_X_train = unnormalised_X[:,selected_sensors]\n",
    "new_y_train = unnormalised_y\n",
    "\n",
    "#standardisation is always needed before training the model\n",
    "scaler_X = StandardScaler().fit(new_X_train)\n",
    "scaler_y = StandardScaler().fit(new_y_train)\n",
    "X = scaler_X.transform(new_X_train)\n",
    "y = scaler_y.transform(new_y_train)\n",
    "\n",
    "# initialization and fit of a Linear model\n",
    "new_model     = linear_model.LinearRegression()\n",
    "new_model.fit(X,y)\n",
    "\n",
    "#predicted output using train data, need to be scaled back\n",
    "pred_y_train  = scaler_y.inverse_transform(new_model.predict(X))\n",
    "\n",
    "#evaluate the training with the mean squared error metric\n",
    "#mse_score_train = mean_squared_error(new_y_train,pred_y_train)\n",
    "mse_score_train = mean_squared_error(new_y_train,pred_y_train)\n",
    "\n",
    "#plot results\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.scatter(new_y_train,pred_y_train)\n",
    "plt.plot([new_y_train.min(),pred_y_train.max()],[new_y_train.min(),pred_y_train.max()],'k:')\n",
    "plt.axis('equal')\n",
    "plt.title(\"TRAIN : mean squared error: {:.3f}\".format(mse_score_train*1000))\n",
    "plt.xlabel('real')\n",
    "plt.ylabel('pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now have a look an the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the features from the test dataset\n",
    "X_test = scaler_X.transform(unnormalised_X_test[:,selected_sensors])\n",
    "\n",
    "#predict the output using the model trained before\n",
    "pred_y_test  = scaler_y.inverse_transform(new_model.predict(X_test))\n",
    "\n",
    "#evaluate the performance of the model on the test dataset\n",
    "mse_score_test = mean_squared_error(unnormalised_y_test,pred_y_test)\n",
    "\n",
    "#plot the results and compare the prediction and evaluation on the train and test datasets\n",
    "plt.figure(figsize=[5,5])\n",
    "plt.scatter(new_y_train,pred_y_train)\n",
    "plt.plot([new_y_train.min(),pred_y_train.max()],[new_y_train.min(),pred_y_train.max()],'k:')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('real')\n",
    "plt.ylabel('pred')\n",
    "plt.scatter(unnormalised_y_test,pred_y_test)\n",
    "plt.plot([unnormalised_y_test.min(),pred_y_test.max()],[unnormalised_y_test.min(),pred_y_test.max()],'k:')\n",
    "plt.axis('equal')\n",
    "plt.title(\"TRAIN : mean squared error: {:.3f}\\nTEST : mean squared error: {:.3f}\".format(mse_score_train*1000,mse_score_test*1000))\n",
    "plt.xlabel('real')\n",
    "plt.ylabel('pred')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
