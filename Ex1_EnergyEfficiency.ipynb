{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loaded-mechanism",
   "metadata": {},
   "source": [
    "# ML Application Example\n",
    "## Regression\n",
    "\n",
    "The task of this example is to implement a complete Data Driven pipeline (load, data-analysis, visualisation, model selection and optimization, prediction) on a specific Dataset. In this example the challenge is to perform a regression with different models to find the most accurate prediction.  \n",
    "\n",
    "<a href=\"https://marcozamana.azurewebsites.net/\"> <img src=\"https://marcozamana.azurewebsites.net/wp-content/uploads/2018/12/pipeline.png\"></a>\n",
    "\n",
    "\n",
    "## Dataset \n",
    "The notebook will upload a public available dataset: https://archive.ics.uci.edu/ml/datasets/energy+efficiency\n",
    "<blockquote>\n",
    "  <b>Source:</b>\n",
    "    The dataset was created by Angeliki Xifara (angxifara@gmail.com, Civil/Structural Engineer) and was processed by Athanasios    \n",
    "    Tsanas (tsanasthanasis@gmail.com, Oxford Centre for Industrial and Applied Mathematics, University of Oxford, UK).\n",
    "    <br/>\n",
    "    <b>Data Set Information:</b>\n",
    "    The author of the dataset perform energy analysis using 12 different building shapes simulated in Ecotect. The buildings differ with \n",
    "    respect to the glazing area, the glazing area distribution, and the orientation, amongst other parameters. They simulate various \n",
    "    settings as functions of the afore-mentioned characteristics to obtain 768 building shapes. The dataset comprises 768 samples and 8 \n",
    "    features, aiming to predict two real valued responses. It can also be used as a multi-class classification problem if the response \n",
    "    is rounded to the nearest integer.\n",
    "    <br/>\n",
    "    <b>Attribute Information:</b>\n",
    "    The dataset contains eight attributes (or features, denoted by X1...X8) and two responses (or outcomes, denoted by y1 and y2). The \n",
    "    aim is to use the eight features to predict each of the two responses.\n",
    "    <br/>\n",
    "    <b>Specifically:</b>\n",
    "    <br/>\n",
    "    <code><br/> X1 :: Relative Compactness <br/> X2 :: Surface Area <br/> X3 :: Wall Area <br/> X4 :: Roof Area </code>\n",
    "    <code><br/> X5 :: Overall Height <br/> X6 :: Orientation <br/> X7 :: Glazing Area <br/> X8 :: Glazing Area Distribution </code>\n",
    "    <code><br/> y1 :: Heating Load <br/> y2 :: Cooling Load </code>\n",
    "    <br/>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-brunei",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algebra\n",
    "import numpy as np\n",
    "# data structure\n",
    "import pandas as pd\n",
    "# data visualization\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "#file handling\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-bankruptcy",
   "metadata": {},
   "source": [
    "# Data load\n",
    "The process consist in downloading the data if needed, loading the data as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "filename  = \"ENB2012_data.xlsx\"\n",
    "separator = ';'\n",
    "columns   = None\n",
    "\n",
    "#if the dataset is not already in the working dir, it will download\n",
    "my_file = Path(filename)\n",
    "if not my_file.is_file():\n",
    "  print(\"Downloading dataset\")\n",
    "  !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ideal-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to semplificate the load of dataset, in case it is a csv, tsv or excel file\n",
    "#output is a pandas dataframe \n",
    "def load_csv(filename,separator,columns):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        csv_table = pd.read_csv(filename,sep=separator,names=columns)\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        csv_table = pd.read_excel(filename,names=columns)\n",
    "    print(\"n. samples: {}\".format(csv_table.shape[0]))\n",
    "    print(\"n. columns: {}\".format(csv_table.shape[1]))\n",
    "\n",
    "    return csv_table #.dropna()\n",
    "\n",
    "data = load_csv(filename,separator,columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-desperate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "colored-library",
   "metadata": {},
   "source": [
    "# Data Analysis and Visualization\n",
    "In this section confidence with the data is gained, data are plotted and cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How does the dataset look like? \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-census",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name of all columns\n",
    "print(data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a look at the data and their correlations, if any\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the correlation matrix\n",
    "corrMatrix = data.corr()\n",
    "\n",
    "plt.figure(figsize=[13,9])\n",
    "sns.heatmap(corrMatrix,annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select only the interesting variable for the model, and remove any anomalous value (e.g. \"nan\")\n",
    "data = data[['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8' ,'Y1', 'Y2']]\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-bacon",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "Here the interesting input features and output to predict for the task are selected, the data are opportunelly preprocessed (i.e. normalized), the dataset is splitted in two separate train and test subsets, each model is trained on the training data and evaluated against a test set. <br/>\n",
    "The evaluation metrics list can be found <a href='https://scikit-learn.org/stable/modules/model_evaluation.html'>here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-chapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the module needed for the modeling and data mining are imported\n",
    "#Cross-Validation \n",
    "from sklearn.model_selection import train_test_split\n",
    "#Data normalization\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "#metrics to evaluate the model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-europe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selection of feature and output variable, definition of the size (fraction of the total) of the random selected test set\n",
    "input_features = ['X1', 'X2', 'X3', 'X4', 'X5', 'X7']\n",
    "output         = ['Y1']\n",
    "test_size      = 0.33\n",
    "random_state   = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not preprocessed data\n",
    "unnormalized_X,unnormalized_y = data[input_features],data[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-independence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation\n",
    "#Having features on a similar scale can help the model converge more quickly towards the minimum\n",
    "scaler_X = StandardScaler().fit(unnormalized_X)\n",
    "scaler_y = StandardScaler().fit(unnormalized_y)\n",
    "X = scaler_X.transform(unnormalized_X)\n",
    "y = scaler_y.transform(unnormalized_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-holly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if nan are present on the data after normalization to avoid trouble later\n",
    "sum(np.isnan(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic train-test dataset random split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-airport",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to help the display of the results\n",
    "Score_Dict = {}\n",
    "\n",
    "#function introduced to simplifies the following comparison and test of the various\n",
    "#return the trained model and the score of the selected metrics\n",
    "def fit_predict_plot(model,X_train,y_train,X_test,y_test):\n",
    "    model.fit(X_train,y_train.ravel())\n",
    "\n",
    "    pred_normalized_y_test = model.predict(X_test)\n",
    "    pred_y_test            = scaler_y.inverse_transform(pred_normalized_y_test)\n",
    "    real_y_test            = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "    mse_score = mean_squared_error(real_y_test,pred_y_test)\n",
    "    \n",
    "    model_name = type(model).__name__\n",
    "    if(model_name=='GridSearchCV'):\n",
    "        model_name ='CV_'+type(model.estimator).__name__\n",
    "\n",
    "    #Alternative metrics are listed here:https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    Score_Dict[model_name]=mse_score\n",
    "\n",
    "    plt.figure(figsize=[5,5])\n",
    "    plt.scatter(real_y_test,pred_y_test)\n",
    "    plt.plot([real_y_test.min(),real_y_test.max()],[real_y_test.min(),real_y_test.max()],'k:')\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"Mean Squared Error: {:.2f}\".format(mse_score))\n",
    "    plt.xlabel('Real Heating Load')\n",
    "    plt.ylabel('Predicted Heating Load')\n",
    "    \n",
    "    return model,mse_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-usage",
   "metadata": {},
   "source": [
    "## Linear models\n",
    "Used linear models in this example are:\n",
    "<ul>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\">Linear Regression</a></li>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\">Lasso</a></li>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\">Ridge</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the module that allows to access the Linear Regression, Lasso and Ridge algorithm\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-jacket",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization, fit and evaluation of the model\n",
    "model = linear_model.LinearRegression()\n",
    "basic_linear_model, basic_linear_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)\n",
    "\n",
    "#check the output of the model\n",
    "print(basic_linear_model.coef_)\n",
    "print(basic_linear_model.intercept_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-crawford",
   "metadata": {},
   "source": [
    "# Ridge\n",
    "For advanced algorithms, hyper-parameters need to be specified, they influence the convergence and the results of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization strength hyper-parameter; must be a positive float\n",
    "alpha = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-camera",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization, fit and evaluation of the model\n",
    "model = linear_model.Ridge(alpha=alpha)\n",
    "ridge_model, ridge_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)\n",
    "\n",
    "#check the output of the model\n",
    "print(ridge_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-pricing",
   "metadata": {},
   "source": [
    "# Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparametr: alpha = 0 is equivalent to an ordinary least square, solved by the LinearRegression\n",
    "alpha = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = linear_model.Lasso(alpha=alpha)\n",
    "lasso_model,lasso_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)\n",
    "\n",
    "print(lasso_model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-animal",
   "metadata": {},
   "source": [
    "# Kernel ridge\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html\">Kernel ridge regression (KRR)</a> combines ridge regression with the kernel trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-peripheral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization hyper-parameter\n",
    "alpha  = 0.01\n",
    "kernel = 'rbf'#'polynomial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-catalog",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelRidge(alpha=alpha,kernel = kernel)\n",
    "krr_model,krr_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-kansas",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\">Epsilon-Support Vector Regression</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-clear",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameter\n",
    "C = 100\n",
    "kernel='rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(C=C,kernel=kernel,gamma='auto')\n",
    "svr_model,svr_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-wealth",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "A <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\">Random Forest Regressor</a> is a meta estimator that fits a number of classifying decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defensive-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators=100)\n",
    "random_forest_model,random_forest_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-retail",
   "metadata": {},
   "source": [
    "# Hyper parameters tuning and Cross Validation\n",
    "Finding the best hyperparameter of interest without writing hundreds of lines of code is an important efficiency gain\n",
    "<br/>CV is to avoid bias in the performance evaluation\n",
    "<br/>\n",
    "For the Tuning a <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\">Grid Search with Cross Validation</a> is used. <br />\n",
    "<code>cv :: Determines the cross-validation splitting strategy.</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-broadway",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#Five fold splitting strategy\n",
    "cv = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-movement",
   "metadata": {},
   "source": [
    "## Ridge with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator  = linear_model.Ridge()\n",
    "parameters = {'alpha':np.logspace(-2,2,10)}\n",
    "model = GridSearchCV(estimator, parameters,cv=cv)\n",
    "\n",
    "cv_ridge_model,cv_ridge_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)\n",
    "print(cv_ridge_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-treasurer",
   "metadata": {},
   "source": [
    "## Lasso with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-roberts",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator  = linear_model.Lasso()\n",
    "parameters = {'alpha':np.linspace(0.001,10,10)}\n",
    "model = GridSearchCV(estimator, parameters,cv=cv)\n",
    "\n",
    "cv_lasso_model,cv_lasso_linear_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)\n",
    "#print the tuned hyper-parameters\n",
    "print(cv_lasso_model.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-tulsa",
   "metadata": {},
   "source": [
    "## KernelRidge with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator  = KernelRidge()\n",
    "parameters = {'kernel':['polynomial','rbf'],\n",
    "              'degree':np.arange(10),\n",
    "              'alpha':np.logspace(-2,2,5)}\n",
    "model = GridSearchCV(estimator, parameters,cv=cv)\n",
    "\n",
    "cv_krr_model,cv_krr_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)\n",
    "print(cv_krr_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-alignment",
   "metadata": {},
   "source": [
    "## Epsilon-Support Vector Regression with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator  = SVR(gamma='auto')\n",
    "parameters = {'kernel':['rbf'],\n",
    "              'C':np.logspace(-2,2,5)}\n",
    "model = GridSearchCV(estimator, parameters,cv=cv)\n",
    "\n",
    "cv_svr_model,cv_svr_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)\n",
    "print(cv_svr_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-assets",
   "metadata": {},
   "source": [
    "## Random Forest Regressor with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-sample",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator  = RandomForestRegressor()\n",
    "parameters = {'n_estimators':[10,100,1000]}\n",
    "model = GridSearchCV(estimator, parameters,cv=cv)\n",
    "\n",
    "cv_rf_model,cv_rf_score = fit_predict_plot(model,X_train,y_train,X_test,y_test)\n",
    "print(cv_rf_model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-submission",
   "metadata": {},
   "source": [
    "## Results comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print out the results in a table\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "table = '<table><tr><th> Model</th><th> Accuracy Metric </th></tr>'\n",
    "\n",
    "for key, value in Score_Dict.items():\n",
    "    table +='<tr> <td>'+key+'</td><td>' +'%.2f'%(value)+'</td></tr>'\n",
    "table+='</table>'\n",
    "display(md(table))\n",
    "\n",
    "\n",
    "names = list(Score_Dict.keys())\n",
    "values = list(Score_Dict.values())\n",
    "\n",
    "plt.figure(figsize=(15, 3))\n",
    "plt.bar(names, values)\n",
    "plt.ylabel('Accuracy Metric')\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-guarantee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
