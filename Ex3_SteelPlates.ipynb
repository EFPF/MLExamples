{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "conditional-agriculture",
   "metadata": {
    "id": "4v77W6thz925"
   },
   "source": [
    "# ML Application Example\n",
    "## Classification using Steel Plates Faults Data Set\n",
    "\n",
    "The task of this example is to implement a complete Data Driven pipeline (load, data-analysis, visualisation, model selection and optimization, prediction) on a specific Dataset. In this example the challenge is to perform a classification with different models to find the most accurate prediction.  \n",
    "\n",
    "\n",
    "## Dataset \n",
    "The notebook will upload a public available dataset: https://archive.ics.uci.edu/ml/datasets/steel+plates+faults\n",
    "<blockquote>\n",
    "  <b>Source:</b>\n",
    "    Semeion, Research Center of Sciences of Communication, Via Sersale 117, 00128, Rome, Italy. www.semeion.it\n",
    "    <br/>\n",
    "    <b>Data Set Information:</b>\n",
    "    Type of dependent variables (7 Types of Steel Plates Faults):\n",
    "    <ol>\n",
    "        <li> Pastry </li> <li> Z_Scratch </li> <li> K_Scatch </li> <li> Stains </li> <li>Dirtiness </li> <li> Bumps</li> <li> Other_Faults</li> </ol> \n",
    "    <br/>\n",
    "    <b>Attribute Information:</b>\n",
    "    27 independent variables:\n",
    "    <table>\n",
    "        <tr><td>X_Minimum        </td><td> X_Maximum           </td><td> Y_Minimum           </td><td>Y_Maximum            </td><td>Pixels_Areas      </td><td>X_Perimeter     </td></tr>\n",
    "        <tr><td>Y_Perimeter      </td><td>Sum_of_Luminosity    </td><td>Minimum_of_Luminosity</td><td>Maximum_of_Luminosity</td><td>Length_of_Conveyer</td><td>TypeOfSteel_A300</td></tr>\n",
    "        <tr><td>TypeOfSteel_A400 </td><td>Steel_Plate_Thickness</td><td>Edges_Index          </td><td>Empty_Index          </td><td>Square_Index      </td><td>Outside_X_Index </td></tr>\n",
    "        <tr><td>Edges_X_Index    </td><td>Edges_Y_Index        </td><td>Outside_Global_Index </td><td>LogOfAreas           </td><td>Log_X_Index       </td><td>Log_Y_Index      </td></tr>\n",
    "        <tr><td>Orientation_Index</td><td>Luminosity_Index     </td><td>SigmoidOfAreas       </td></tr></table> \n",
    "    <br/>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-bacteria",
   "metadata": {
    "id": "gtI3Kg4Sz93C"
   },
   "outputs": [],
   "source": [
    "# algebra\n",
    "import numpy as np\n",
    "# data structure\n",
    "import pandas as pd\n",
    "# data visualization\n",
    "import matplotlib.pylab as plt\n",
    "# another module for data visualization\n",
    "import plotly.express as px\n",
    "\n",
    "import seaborn as sns\n",
    "#file handling\n",
    "from pathlib import Path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-estate",
   "metadata": {
    "id": "QuU41y4fz93D"
   },
   "source": [
    "# Data load\n",
    "The process consist in downloading the data if needed, loading the data as a Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-casting",
   "metadata": {
    "id": "KUBlI0POz93H"
   },
   "outputs": [],
   "source": [
    "    \n",
    "filename  = \"Faults.NNA\"\n",
    "separator = '\\t'\n",
    "columns   = ['X_Minimum','X_Maximum','Y_Minimum','Y_Maximum','Pixels_Areas','X_Perimeter','Y_Perimeter','Sum_of_Luminosity','Minimum_of_Luminosity','Maximum_of_Luminosity','Length_of_Conveyer',\n",
    "             'TypeOfSteel_A300','TypeOfSteel_A400','Steel_Plate_Thickness','Edges_Index','Empty_Index','Square_Index','Outside_X_Index','Edges_X_Index','Edges_Y_Index','Outside_Global_Index',\n",
    "            'LogOfAreas','Log_X_Index','Log_Y_Index','Orientation_Index','Luminosity_Index','SigmoidOfAreas','Pastry','Z_Scratch','K_Scatch','Stains','Dirtiness','Bumps','Other_Faults']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-rotation",
   "metadata": {
    "id": "7VtB6M9UeHYw"
   },
   "outputs": [],
   "source": [
    "#if the dataset is not already in the working dir, it will download\n",
    "my_file = Path(filename)\n",
    "if not my_file.is_file():\n",
    "  print(\"Downloading dataset\")\n",
    "  !wget https://archive.ics.uci.edu/ml/machine-learning-databases/00198/Faults.NNA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-cooperative",
   "metadata": {
    "id": "2PGteJ_Cz93J"
   },
   "outputs": [],
   "source": [
    "#function to semplificate the load of dataset, in case it is a csv, tsv or excel file\n",
    "#output is a pandas dataframe \n",
    "def load_csv(filename,separator,columns):\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        csv_table = pd.read_csv(filename,sep=separator,names=columns,dtype='float64')\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        csv_table = pd.read_excel(filename,names=columns)\n",
    "    print(\"n. samples: {}\".format(csv_table.shape[0]))\n",
    "    print(\"n. columns: {}\".format(csv_table.shape[1]))\n",
    "\n",
    "    return csv_table #.dropna()\n",
    "\n",
    "data = load_csv(filename,separator,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-castle",
   "metadata": {
    "id": "lJfmyaCQz93M"
   },
   "source": [
    "# Data Analysis and Visualization\n",
    "In this section confidence with the data is gained, data are plotted and cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-jungle",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "Q-U1XUpUz93N",
    "outputId": "ff024a03-c91f-4446-88f6-fefd1b13f9a8"
   },
   "outputs": [],
   "source": [
    "#How does the dataset look like? \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-retro",
   "metadata": {
    "id": "wFNSThFP2vQt"
   },
   "outputs": [],
   "source": [
    "Faults = ['Pastry',\n",
    "'Z_Scratch',\n",
    "'K_Scatch',\n",
    "'Stains',\n",
    "'Dirtiness',\n",
    "'Bumps',\n",
    "'Other_Faults']\n",
    "\n",
    "data['class'] = (data[Faults]*np.arange(len(Faults))).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-ireland",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "HGOgg9hX20fO",
    "outputId": "f9229e5b-569a-497e-d61d-8673aae2d528"
   },
   "outputs": [],
   "source": [
    "#Do we have a balanced dataset?\n",
    "plt.bar(Faults,data[Faults].sum())\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-metallic",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3rJOCrBqz93O",
    "outputId": "679da9a6-8357-45fc-cf33-85944690bd99"
   },
   "outputs": [],
   "source": [
    "#Name of all columns\n",
    "print(data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-benjamin",
   "metadata": {
    "id": "ohfcOe-9z93P"
   },
   "outputs": [],
   "source": [
    "#let's have a look at the data and their correlations, if any\n",
    "measurements = ['X_Minimum',\n",
    "            'X_Maximum',\n",
    "            'Y_Minimum',\n",
    "            'Y_Maximum',\n",
    "            'Pixels_Areas',\n",
    "            'X_Perimeter',\n",
    "            'Y_Perimeter',\n",
    "            'Sum_of_Luminosity',\n",
    "            'Minimum_of_Luminosity',\n",
    "            'Maximum_of_Luminosity',\n",
    "            'Length_of_Conveyer',\n",
    "            'TypeOfSteel_A300',\n",
    "            'TypeOfSteel_A400',\n",
    "            'Steel_Plate_Thickness',\n",
    "            'Edges_Index',\n",
    "            'Empty_Index',\n",
    "            'Square_Index',\n",
    "            'Outside_X_Index',\n",
    "            'Edges_X_Index',\n",
    "            'Edges_Y_Index',\n",
    "            'Outside_Global_Index',\n",
    "            'LogOfAreas',\n",
    "            'Log_X_Index',\n",
    "            'Log_Y_Index',\n",
    "            'Orientation_Index',\n",
    "            'Luminosity_Index',\n",
    "            'SigmoidOfAreas']\n",
    "target       = ['class']\n",
    "\n",
    "#let's have a look only at a few parameters\n",
    "sns.pairplot(data[['X_Minimum', 'X_Maximum', 'Y_Minimum', 'Y_Maximum', 'Pixels_Areas', 'X_Perimeter', 'Y_Perimeter', 'Sum_of_Luminosity', 'Minimum_of_Luminosity', 'Maximum_of_Luminosity']+target],hue='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-method",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "NqLfNNn5UPng",
    "outputId": "6c9c5651-a7eb-496f-9747-f0ef32ed3987"
   },
   "outputs": [],
   "source": [
    "#Let's have a look if there is the possibility to reduce the dimensionality\n",
    "#to see if there is the possibility to see if the fault-classes are \"separable\" \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "aux = data[measurements]\n",
    "aux = (aux-aux.mean())/aux.std()\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "X_r = pca.fit(aux).transform(aux)\n",
    "y_r   = data[target].values.flatten()\n",
    "\n",
    "\n",
    "colors = plt.cm.get_cmap('Dark2')(np.linspace(0,1,len(Faults)))\n",
    "lw = 2\n",
    "\n",
    "fig = plt.figure(figsize=[10,10])\n",
    "\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(X_r[:,0], X_r[:,1], X_r[:,2], c=data[target].values, cmap='viridis', linewidth=0.5);\n",
    "\n",
    "#print(data[target].column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another fancy way of doing the previous plot\n",
    "px.scatter_3d( x=X_r[:,0], y=X_r[:,1], z=X_r[:, 2], color=data['class'].values,color_continuous_scale='Rainbow' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-decimal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "h_Hb2efXVEFm",
    "outputId": "16617fa3-878f-425e-b5e3-c90c404776ce"
   },
   "outputs": [],
   "source": [
    "#t-distributed stochastic neighbor embedding is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=3)\n",
    "X_r = tsne.fit_transform(aux)\n",
    "y_r  = data[target].values.flatten()\n",
    "\n",
    "\n",
    "#colors = plt.cm.get_cmap('viridis')(np.linspace(0,1,len(Faults)))\n",
    "#lw = 2\n",
    "\n",
    "fig = plt.figure(figsize=[10,10])\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(X_r[:,0], X_r[:,1], X_r[:,2], c=data[target].values, cmap='viridis', linewidth=0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-lyric",
   "metadata": {
    "id": "JYPSvPSHz93S"
   },
   "outputs": [],
   "source": [
    "#Select only the interesting variable for the model, and remove any anomalous value (e.g. \"nan\")\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-indie",
   "metadata": {
    "id": "gpNTX-7jz93U"
   },
   "source": [
    "# Machine Learning\n",
    "Here the interesting input features and output to predict for the task are selected, the data are opportunelly preprocessed (i.e. normalized), the dataset is splitted in two separate train and test subsets, each model is trained on the training data and evaluated against a test set. <br/>\n",
    "The evaluation metrics list can be found <a href='https://scikit-learn.org/stable/modules/model_evaluation.html'>here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-opinion",
   "metadata": {
    "id": "LEC0gXyzz93V"
   },
   "outputs": [],
   "source": [
    "#the module needed for the modeling and data mining are imported\n",
    "#Cross-Validation \n",
    "from sklearn.model_selection import train_test_split\n",
    "#Data normalization\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "#metrics to evaluate the model\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-dealing",
   "metadata": {
    "id": "L3pYFttfz93W"
   },
   "outputs": [],
   "source": [
    "#Selection of feature and output variable, definition of the size (fraction of the total) of the random selected test set\n",
    "input_features = measurements\n",
    "output         = target\n",
    "test_size      = 0.33\n",
    "random_state   = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gothic-willow",
   "metadata": {
    "id": "d5sw1PnUz93X"
   },
   "outputs": [],
   "source": [
    "#not preprocessed data\n",
    "unnormalized_X,y = data[input_features],data[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-treat",
   "metadata": {
    "id": "RpGO5CgCz93X"
   },
   "outputs": [],
   "source": [
    "# normalisation\n",
    "#Having features on a similar scale can help the model converge more quickly towards the minimum\n",
    "scaler_X = StandardScaler().fit(unnormalized_X)\n",
    "X = scaler_X.transform(unnormalized_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-appeal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6UhlUTQz93Y",
    "outputId": "06614070-6221-42a7-cc56-31ee924106e0"
   },
   "outputs": [],
   "source": [
    "#check if nan are present on the data after normalization to avoid trouble later\n",
    "sum(np.isnan(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imposed-atlas",
   "metadata": {
    "id": "Wchsv209z93Y"
   },
   "outputs": [],
   "source": [
    "# basic train-test dataset random split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-yorkshire",
   "metadata": {
    "id": "n3yXv5kzz93Z"
   },
   "outputs": [],
   "source": [
    "#dictionary to help the display of the results\n",
    "Score_Dict = {}\n",
    "\n",
    "#function introduced to simplifies the following comparison and test of the various\n",
    "#return the trained model and the score of the selected metrics\n",
    "def fit_predict_plot(model,X_train,y_train,X_test,y_test,class_names):\n",
    "    model.fit(X_train,y_train)\n",
    "\n",
    "    pred_y_test = model.predict(X_test)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_test,pred_y_test)\n",
    "    score = f1_score(y_test,pred_y_test,average='weighted')\n",
    "\n",
    "    model_name = type(model).__name__\n",
    "    if(model_name=='GridSearchCV'):\n",
    "        model_name ='CV_'+type(model.estimator).__name__\n",
    "\n",
    "    #Alternative metrics are listed here:https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    Score_Dict[model_name]=score\n",
    "\n",
    "    fig,ax = plt.subplots(1,1,figsize=[10,10])\n",
    "    \n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    plot_confusion_matrix(model,X_test,y_test,display_labels=class_names,\n",
    "                                 cmap     =plt.cm.Blues,\n",
    "                                 normalize='true',\n",
    "                                 xticks_rotation=45,ax=ax)\n",
    "    plt.axis('tight')\n",
    "    \n",
    "    correctly_classified = np.sum(np.diag(conf_matrix))/np.sum(conf_matrix)\n",
    "    print(\"correctly classified :: {:.2f}\".format(correctly_classified))\n",
    "    print(\"f1 score :: {:.2f}\".format(score))\n",
    "    \n",
    "    \n",
    "    return model,correctly_classified\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-point",
   "metadata": {
    "id": "6sLY8EmRz93a"
   },
   "source": [
    "## Models used in this example are:\n",
    "<ul>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html#sklearn.linear_model.RidgeClassifier\">Ridge</a></li>\n",
    "     <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression\">Logistic Regression</a></li>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier\">kNN</a></li>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\">Support Vector Classification</a></li>\n",
    "    <li><a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">Random Forest</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-latter",
   "metadata": {
    "id": "S16NCi1RL7bj"
   },
   "source": [
    "# Ridge Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-external",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "hSvW_dDez93a",
    "outputId": "d4bc0924-cc3b-47aa-92e3-cf38750e993c"
   },
   "outputs": [],
   "source": [
    "#initialization, fit and evaluation of the model\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "estimator = RidgeClassifier()\n",
    "\n",
    "parameters = { 'alpha':np.logspace(-2,2,5)}\n",
    "model = GridSearchCV(estimator, parameters,cv=5)\n",
    "\n",
    "model, ridge_score = fit_predict_plot(model,X_train,y_train.values.flatten(),X_test,y_test.values.flatten(),Faults)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-dover",
   "metadata": {
    "id": "canOqoWHMY9P"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-floating",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "7qrkXN-cIrFf",
    "outputId": "090b6e7f-af1c-4ca6-89e7-928eb5c0a7fb"
   },
   "outputs": [],
   "source": [
    "#initialization, fit and evaluation of the model\n",
    "from sklearn import linear_model\n",
    "estimator = linear_model.LogisticRegression(max_iter=10000)\n",
    "\n",
    "parameters = { 'C':np.logspace(-2,3,5)}\n",
    "model = GridSearchCV(estimator, parameters,cv=5)\n",
    "\n",
    "model, logistic_score = fit_predict_plot(model,X_train,y_train.values.flatten(),X_test,y_test.values.flatten(),Faults)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-mongolia",
   "metadata": {
    "id": "p_N_eVk5Meib"
   },
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-campaign",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "id": "bfbnIFGEJiFP",
    "outputId": "892be221-1f48-4163-c888-ab63ae9ae9ed"
   },
   "outputs": [],
   "source": [
    "#initialization, fit and evaluation of the model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "estimator = KNeighborsClassifier()\n",
    "\n",
    "parameters = { 'n_neighbors':[3,5,7]}\n",
    "model = GridSearchCV(estimator, parameters,cv=5)\n",
    "\n",
    "model, knn_score = fit_predict_plot(model,X_train,y_train.values.flatten(),X_test,y_test.values.flatten(),Faults)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-density",
   "metadata": {
    "id": "Gdybde6aMn95"
   },
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-converter",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "gVpTPdm8Mnfb",
    "outputId": "30280482-cacf-41ad-930f-c9cf46867c59"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "estimator = SVC(gamma='auto')\n",
    "\n",
    "parameters = { 'C':[0.1,1,10,100]}\n",
    "model = GridSearchCV(estimator, parameters,cv=5)\n",
    "\n",
    "model, svc_score = fit_predict_plot(model,X_train,y_train.values.flatten(),X_test,y_test.values.flatten(),Faults)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-scoop",
   "metadata": {
    "id": "0s1y2od0MhxU"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-modification",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "sogpyC7hIREr",
    "outputId": "422ccc4f-ba11-4fee-e9c0-ea140708e578"
   },
   "outputs": [],
   "source": [
    "#initialization, fit and evaluation of the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "parameters = { 'min_samples_leaf':[1,3,5],\n",
    "              'class_weight':['balanced_subsample'],\n",
    "              'n_estimators':[10,100,200]}\n",
    "model = GridSearchCV(estimator, parameters,cv=5)\n",
    "\n",
    "model, rf_score = fit_predict_plot(model,X_train,y_train.values.flatten(),X_test,y_test.values.flatten(),Faults)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-spring",
   "metadata": {
    "id": "dmM95YkGz93s",
    "outputId": "5fb2e4bb-d82a-44be-a1e3-bf719966cf74"
   },
   "outputs": [],
   "source": [
    "#print out the results in a table\n",
    "from IPython.display import Markdown as md\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "table = '<table><tr><th> Model</th><th> Accuracy Metric </th></tr>'\n",
    "\n",
    "for key, value in Score_Dict.items():\n",
    "    table +='<tr> <td>'+key+'</td><td>' +'%.2f'%(value)+'</td></tr>'\n",
    "table+='</table>'\n",
    "display(md(table))\n",
    "\n",
    "\n",
    "names = list(Score_Dict.keys())\n",
    "values = list(Score_Dict.values())\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.bar(names, values)\n",
    "plt.ylabel('Accuracy Metric')\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid()\n",
    "#plt.ylim([0.5,0.8])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-criterion",
   "metadata": {},
   "source": [
    "# How to deal with Unbalanced dataset\n",
    "There are at least two possibilities as explaned <a href=\"https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\">here</a> and <a href=\"https://medium.com/strands-tech-corner/unbalanced-datasets-what-to-do-144e0552d9cd\"> here </a>: \n",
    "<b> Undersampling or Oversampling</b> \n",
    "\n",
    "<img  width=\"500\" src=\"https://miro.medium.com/max/2400/1*ENvt_PTaH5v4BXZfd-3pMA.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-mainstream",
   "metadata": {
    "id": "MV0U7i-Iz93u"
   },
   "outputs": [],
   "source": [
    "#Undersample\n",
    "counts = data[target].value_counts()\n",
    "mincounts = np.min(counts)\n",
    "\n",
    "\n",
    "df = [0,0,0,0,0,0,0]\n",
    "df_under = pd.DataFrame()\n",
    "for a in range(len(Faults)):\n",
    "    df = data.loc[data[Faults[a]]==1].sample(mincounts) \n",
    "    df_under = pd.concat([df_under, df], axis=0)\n",
    "    \n",
    "    \n",
    "plt.bar(Faults,df_under[Faults].sum())\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not preprocessed data\n",
    "unnormalized_X,y = df_under[input_features],df_under[output]\n",
    "# normalisation\n",
    "#Having features on a similar scale can help the model converge more quickly towards the minimum\n",
    "scaler_X = StandardScaler().fit(unnormalized_X)\n",
    "X = scaler_X.transform(unnormalized_X)\n",
    "\n",
    "# basic train-test dataset random split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "#initialization, fit and evaluation of the model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "estimator = KNeighborsClassifier()\n",
    "\n",
    "parameters = { 'n_neighbors':[3,5,7,9]}\n",
    "model = GridSearchCV(estimator, parameters,cv=5)\n",
    "\n",
    "model, knn_score = fit_predict_plot(model,X_train,y_train.values.flatten(),X_test,y_test.values.flatten(),Faults)\n",
    "print(model.best_params_)\n",
    "\n",
    "#initialization, fit and evaluation of the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "parameters = { 'min_samples_leaf':[1,3,5],\n",
    "              'class_weight':['balanced_subsample'],\n",
    "              'n_estimators':[10,100,200,300]}\n",
    "model = GridSearchCV(estimator, parameters,cv=5)\n",
    "\n",
    "model, rf_score = fit_predict_plot(model,X_train,y_train.values.flatten(),X_test,y_test.values.flatten(),Faults)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversample \n",
    "counts = data[target].value_counts()\n",
    "maxcounts = np.max(counts)\n",
    "\n",
    "df_over = pd.DataFrame()\n",
    "for a in range(len(Faults)):\n",
    "    df = data.loc[data[Faults[a]]==1].sample(maxcounts,replace=True) \n",
    "    df_over = pd.concat([df_over, df], axis=0)\n",
    "    \n",
    "    \n",
    "plt.bar(Faults,df_over[Faults].sum())\n",
    "plt.xticks(rotation=30)\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not preprocessed data\n",
    "unnormalized_X,y = df_over[input_features],df_over[output]\n",
    "# normalisation\n",
    "#Having features on a similar scale can help the model converge more quickly towards the minimum\n",
    "scaler_X = StandardScaler().fit(unnormalized_X)\n",
    "X = scaler_X.transform(unnormalized_X)\n",
    "\n",
    "# basic train-test dataset random split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=test_size,\n",
    "                                                    random_state=random_state)\n",
    "\n",
    "#initialization, fit and evaluation of the model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "estimator = KNeighborsClassifier()\n",
    "\n",
    "parameters = { 'n_neighbors':[3,5,7,9]}\n",
    "model = GridSearchCV(estimator, parameters,cv=5)\n",
    "\n",
    "model, knn_score = fit_predict_plot(model,X_train,y_train.values.flatten(),X_test,y_test.values.flatten(),Faults)\n",
    "print(model.best_params_)\n",
    "\n",
    "#initialization, fit and evaluation of the model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "estimator = RandomForestClassifier()\n",
    "\n",
    "parameters = { 'min_samples_leaf':[1,3,5],\n",
    "              'class_weight':['balanced_subsample'],\n",
    "              'n_estimators':[10,100,200,300]}\n",
    "model = GridSearchCV(estimator, parameters,cv=5)\n",
    "\n",
    "model, rf_score = fit_predict_plot(model,X_train,y_train.values.flatten(),X_test,y_test.values.flatten(),Faults)\n",
    "print(model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documentary-southwest",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-roman",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ex1_EnergyEfficiency.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
